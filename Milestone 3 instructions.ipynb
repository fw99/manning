{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 \n",
    " \n",
    "#### Objective:\n",
    "\n",
    " - Build a model with algorithmic gender bias. Using SHAP, AIF-360, and manual calculations, detect algorithmic bias. Then, using AIF-360, mitigate the bias you detected.\n",
    "\n",
    "#### Workflow:\n",
    "\n",
    " - Load data\n",
    " - Train a model without unwanted gender bias\n",
    " - Demonstrate using confusion matrices that the model is not significantly biased\n",
    " - Manually measure equal opportuity difference and disparate impact\n",
    " - Introduce bias into the data\n",
    " - Train a biased model\n",
    " - Demonstrate using confusion matrices that the model is significantly biased\n",
    " - Manually measure equal opportunity difference and disparate impact\n",
    " - Use SHAP explainer to explain the model's underlying predictions\n",
    " - Load the dataset into AIF360\n",
    " - Debias the model using Reweighing and measure the remaining bias\n",
    " - Debias the model using Disparate Impact Remover and measure the remaining bias\n",
    " - Debias the model using Calibrated Equal Odds Postprocessing and measure the remaining bias\n",
    " - Choose one of the approaches, recommend it for use, and explain why\n",
    " - Fine-tune your Reweighing and Disparate Impact Remover approaches by selecting the best possible threshold\n",
    "\n",
    "#### Importance to Project:\n",
    "\n",
    " - In the scenario of this LiveProject, you are called upon to detect and/or mitigate unwanted bias in three models (and their corresponding datasets) for the WHO. Here we work with two models - one which probably does not have unwanted bias, and one which does. In these cases, the protected class data is available. For our final model, in Milestone 4, it is not available. \n",
    " - Most of the work here is done on the model which does have unwanted bias. We want to measure that bias and try and mitigate it. The workflow guides you through that process.\n",
    " \n",
    "#### Resources:\n",
    " - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6678298/ (a description of the original dataset and its use)\n",
    " - https://arxiv.org/pdf/1810.01943.pdf (the original AIF360 paper, which describes several methods for debiasing implemented within the package)\n",
    " - https://aif360.mybluemix.net/data (a demo of AIF360. Here, we are unpacking what is done at the \"check\" and \"mitigate\" steps.) \n",
    " - https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb (a helpful summary of key metrics. Section 4.7 on impossibility is relevant for the final section of this milestone.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules, register helper functions, and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using is a cardiology dataset described in the [Journal of Clinical Medicine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6678298/). In the original dataset the 'cardio' column represented whether or not the patient has cardiovascular disease. \n",
    "    \n",
    "In our scenario, we can obtain all of the measurements in the dataset but we cannot detect cardiovascular disease directly. This is where a model is used. A patient which has a prediction of cardiovascular disease will be given a medication which reduces the likelihood of a cardiac event, but not enough medication is available to give to everyone, so our model will also be used to prioritize the allocation of that medicine.\n",
    "\n",
    "To introduce you to the data, lets' do the following;\n",
    " - import the relevant modules and helper functions\n",
    " - load the data from 'cardio_train.csv',\n",
    " - drop nan values\n",
    " - sort the data by 'cardio', the target variable\n",
    " - inspect the dataframe, and familiarize yourself with the labels (see table 2 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BlackBoxAuditing in c:\\users\\malta\\anaconda3\\lib\\site-packages (0.1.54)\n",
      "Requirement already satisfied: networkx in c:\\users\\malta\\anaconda3\\lib\\site-packages (from BlackBoxAuditing) (2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\malta\\anaconda3\\lib\\site-packages (from BlackBoxAuditing) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\malta\\anaconda3\\lib\\site-packages (from BlackBoxAuditing) (3.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\malta\\anaconda3\\lib\\site-packages (from BlackBoxAuditing) (1.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from networkx->BlackBoxAuditing) (4.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from pandas->BlackBoxAuditing) (2019.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\malta\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->BlackBoxAuditing) (45.2.0.post20200210)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->BlackBoxAuditing) (1.14.0)\n",
      "Requirement already satisfied: aif360 in c:\\users\\malta\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from aif360) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from aif360) (0.23.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\malta\\anaconda3\\lib\\site-packages (from aif360) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from aif360) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from aif360) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21->aif360) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21->aif360) (2.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->aif360) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->aif360) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->aif360) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from matplotlib->aif360) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\malta\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->aif360) (2019.3)\n",
      "Requirement already satisfied: six in c:\\users\\malta\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->aif360) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\malta\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->aif360) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt    \n",
    "import numpy as np\n",
    "import shap\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import aif360\n",
    "from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.explainers import MetricTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biased_dataset(dataset, women_yes_event_subsample_rate):\n",
    "    \n",
    "    \"\"\" (DataFrame, float) -> DataFrame\n",
    "    Using the Cardio dataset, subsample positive cardiac events for women\"\"\"\n",
    "    \n",
    "    women_no_event = dataset[(dataset['gender']==1) & (dataset['cardio']==0)]\n",
    "    women_yes_event = dataset[(dataset['gender']==1) & (dataset['cardio']==1)]\n",
    "    men_no_event = dataset[(dataset['gender']==2) & (dataset['cardio']==0)]\n",
    "    men_yes_event = dataset[(dataset['gender']==2) & (dataset['cardio']==1)]\n",
    "\n",
    "    biased_dataset = pd.concat([\n",
    "        women_no_event,\n",
    "        women_yes_event.sample(int(len(women_yes_event)*women_yes_event_subsample_rate)),\n",
    "        men_no_event,\n",
    "        men_yes_event\n",
    "    ])\n",
    "    return biased_dataset\n",
    "\n",
    "def train_model(dataset, exclude_gender = False):\n",
    "    \n",
    "    \"\"\" (DataFrame, bool) -> RandomForestClassifier, DataFrame, DataFrame, DataFrame, DataFrame\n",
    "    Given a dataset in Cardio format, split the model into train and test, and train a Random Forest classifier on the data\"\"\"\n",
    "    \n",
    "    X_cols = [c for c in dataset.columns if c!='cardio']\n",
    "    X = dataset[X_cols]\n",
    "    y = dataset['cardio']\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y, random_state=42)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    if exclude_gender:\n",
    "        X_cols.remove('gender')\n",
    "    rf.fit(X_train[X_cols], y_train)\n",
    "        \n",
    "    return rf, X_cols, X_train, X_test, y_train, y_test\n",
    "\n",
    " \n",
    "\n",
    "def plot_confusion_matrix(y_true,y_pred, return_percentage=False):\n",
    "    \n",
    "    \"\"\" (Series, Series, bool) -> None\n",
    "    Plot a confusion matrix\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true,y_pred,labels=[1,0])\n",
    "    \n",
    "    if return_percentage:\n",
    "        cm = np.round(np.asarray(cm)/(np.asarray(cm).sum()),3)\n",
    "\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Greens',fmt='g'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['positive', 'negative']); ax.yaxis.set_ticklabels(['positive', 'negative']);\n",
    "    plt.show()\n",
    "    \n",
    "def aif360_to_pandas(dataset_aif360):\n",
    "    return dataset.convert_to_dataframe()[0]\n",
    "def pandas_to_aif360(dataset_pd):\n",
    "    return aif360.datasets.BinaryLabelDataset(1,0,df=dataset_pd,label_names=['cardio'],protected_attribute_names=['gender'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/cardio_train.csv',delimiter=';')\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset.sort_values('cardio',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train a model without apparent gender bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model trained from this data will not be subject to significant bias. However, being able to recognize when a model isn't biased is an important part of the debiasing process.\n",
    "\n",
    "Code is provided to do the following steps:\n",
    "\n",
    " - Using the helper function train_model, train a model\n",
    " - Using the helper function plot_confusion_matrix, plot a confusion matrix based on the model's predictions for the test set.\n",
    " - Split the patients in the test set by gender, and create a confusion matrix for each. \n",
    " \n",
    "Based on the confusion matrices generated above, you are asked to:\n",
    "\n",
    " - Manually measure the disparate impact by dividing the ratio of positive predictions for females by the ratio of positive predictions for males. There should be negligible bias (disparate impact is close to 1).\n",
    " - Manually measure the equal opportunity difference by subtrating the true positive rate for males, from the true positive rate for females. There should be negligible bias (equal opportunity difference is close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rf, X_cols,X_train, X_test, y_train, y_test = train_model(dataset)\n",
    "plot_confusion_matrix(y_test, original_rf.predict(X_test[X_cols]))\n",
    "plot_confusion_matrix(y_test, original_rf.predict(X_test[X_cols]),return_percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_test_idx = (X_test['gender']==1)\n",
    "men_test_idx = (X_test['gender']==2)\n",
    "print('confusion matrix for women in test')\n",
    "plot_confusion_matrix(y_test[women_test_idx], original_rf.predict(X_test[X_cols][women_test_idx]))\n",
    "plot_confusion_matrix(y_test[women_test_idx], original_rf.predict(X_test[X_cols][women_test_idx]),return_percentage=True)\n",
    "print('confusion matrix for men in test')\n",
    "plot_confusion_matrix(y_test[men_test_idx], original_rf.predict(X_test[X_cols][men_test_idx]))\n",
    "plot_confusion_matrix(y_test[men_test_idx], original_rf.predict(X_test[X_cols][men_test_idx]),return_percentage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place your disparate impact and equal opportunity difference measurements here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing gender bias into data \n",
    "\n",
    "We can introduce gender bias into the data by manipulating the ratio of positive to negative events for either gender. In this case, we will alter the dataset by removing a percentage% of the occurrences where women have cardiovascular disease. In other words, on the altered data, women will have a lower likelihood of cardiovascular disease.\n",
    "\n",
    "Code is provided to do the following steps:\n",
    "\n",
    " - Create the biased dataset using the generate_biased_dataset helper function. **By default, 40% of cardiac events for women are removed, creating a gender imbalance**.\n",
    " - Train a model based on this biased dataset using the train_model helper function\n",
    " - Create a confusion matrix using the plot_confusion_matrix helper function\n",
    " - Create a confusion matrix for the subset of the test set where the gender is male, and a confusion matrix for the subset of the test set where gender is female.\n",
    "\n",
    "Based on the confusion matrices generated above, you are asked to:\n",
    " - Manually measure the disparate impact by dividing the ratio of positive predictions for females by the ratio of positive predictions for males. There should be negligible bias (disparate impact is close to 1).\n",
    " - Manually measure the equal opportunity difference by subtrating the true positive rate for females, from the true positive rate for males (there should be a difference around 10%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_dataset = generate_biased_dataset(dataset, women_yes_event_subsample_rate = 0.6)\n",
    "\n",
    "biased_rf, X_cols,X_train, X_test, y_train, y_test = train_model(biased_dataset)\n",
    "plot_confusion_matrix(y_test, biased_rf.predict(X_test[X_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_test_idx = (X_test['gender']==1)\n",
    "men_test_idx = (X_test['gender']==2)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    y_test[women_test_idx], \n",
    "    biased_rf.predict(X_test[X_cols][women_test_idx]))\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    y_test[women_test_idx], \n",
    "    biased_rf.predict(X_test[X_cols][women_test_idx]),return_percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    y_test[men_test_idx], \n",
    "    biased_rf.predict(X_test[X_cols][men_test_idx]),)\n",
    "plot_confusion_matrix(\n",
    "    y_test[men_test_idx], \n",
    "    biased_rf.predict(X_test[X_cols][men_test_idx]),return_percentage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place your disparate impact and equal opportunity difference measurements here¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing gender bias in the model using SHAP\n",
    "\n",
    " - It is relatively simple to show that the model is considering gender by creating a SHAP summary plot, as we learnt in milestone 1. \n",
    " - If we were looking at this model for the first time, without prior knowledge that it had gender bias, seeing gender as a very important feature would be a red flag.\n",
    " - It is not possible to determine from the SHAP plot if the gender feature is given high importance because of genuine physiological differences, or if unwanted algorithmic bias is present.\n",
    "\n",
    "Generate a SHAP summary plot explaining the biased model. You learned this process in Milestone 1, but there is one difference here, because you are interpreting a classifier instead of a regressor.\n",
    "\n",
    "Regressor: `shap.summary_plot(shap_values, X_test[:50])`\n",
    "\n",
    "Classifier: `shap.summary_plot(shap_values[1], X_test[:50])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why can't we just remove the 'gender' variable'?\n",
    "An interesting question: If the SHAP value is indicating that the 'gender' variable has some importance, and we don't want it to be a source of bias, could we simply remove it rather than going through a time-consuming debiasing process? \n",
    "\n",
    "Unfortunately, it is usually not so simple. In this instance, like many others, the model will be able to approximate gender using other variables. Removing the variable also can reduce the accuracy of the model. \n",
    "\n",
    "As an optional exercise, if you are curious, you can train a model without the 'gender' variable and confirm that removing the gender variable does not resolve all the algorithmic bias issues. You can pass in __exclude_gender=True__ to the train_model helper function. Repeat the previous process of generating confusion matrices and manually calculating disparate impact and equal opportunity difference. Though there is less bias, it is not removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing our model with AIF360\n",
    "\n",
    "We have now manually observed bias which might be unwanted. We should nail down exactly what these biases are. There are at least two obvious biases in the model we built:\n",
    " 1. Due directly to our tampering with the dataset, the model predicts more positive cases for men than women. **The relevant bias metric here is disparate impact.**\n",
    " 2. The model's errors are distributed unevenly across genders: women have a lower true positive rater than men. **The relevant bias metric here is equal opportunity difference.**\n",
    "\n",
    "The question remains which of these biases are unwanted algorithmic biases, and how we could mitigate them \n",
    "\n",
    "1. In this milestone, we artifically caused disparate impact **(1)**. However, disparities between men and women can be due to general physiological differences (for example, breast cancer is [70-100 times less common in men than in women](https://www.cancer.org/cancer/breast-cancer-in-men/about/key-statistics.html). With more knowledge about the experimental setup, a subject matter expert in cardiovascular disease would be able to give a qualified answer about the source of the bias and whether or not it's unwanted. For learning purposes, we will assume that this bias is unwanted and attempt to remove it.\n",
    "\n",
    "2. The uneven distribution of errors **(2)** is more straightforward. The model errors are unfairly distributed in such a way that borderline cases in women are more likely to be judged as negative than borderline cases in men. If we can resolve this without a substantial drop in overall accuracy, that would usually be the correct choice.\n",
    "\n",
    "For learning purposes, we will attempt to mitigate both types of bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing steps\n",
    " - Load train and test datasets into AIF360\n",
    " - Using ClassificationMetric, measure key bias metrics (accuracy, disparate impact, equal opportunity difference) in the biased model's predictions\n",
    " - Use each of the following debias methods \n",
    "  - a pre-processing debiasing method (eg reweighing), \n",
    "  - a processing debiasing method (eg prejudice remover)\n",
    "  - a post-processing debiasing method (eg calibration)\n",
    " - Decide on a preferred debiasing approach\n",
    " - Using your reweighing dataset, \n",
    " \n",
    "For demonstration purposes, the below cell contains an audit of accuracy, statistical parity difference, and disparate impact, with respect to the model and the biased dataset. You can modify the code to report on relevant metrics for each debiasing method you choose to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load biased dataset into AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aif360_biased_dataset_train = pandas_to_aif360(pd.concat([X_train, y_train], axis=1)) #this is a helper function, you could insetad use the below\n",
    "# aif360_biased_dataset = aif360.datasets.BinaryLabelDataset(1,0,df=pd.concat([X_train, y_train], axis=1),label_names=['cardio'],protected_attribute_names=['gender'])\n",
    "aif360_biased_dataset_test = pandas_to_aif360(pd.concat([X_test, y_test], axis=1))\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding AIF360 Datasets and ClassificationMetrics\n",
    "\n",
    "For our purposes, AIF360 BinaryLabelDatasets have four key components:\n",
    "\n",
    "\n",
    "| Component  | Code  | Description  | \n",
    "| --- | --- | ---\n",
    "| Features | BinaryLabelDataset.features  |  features (usually inputs to a model).\n",
    "| Scores  | BinaryLabelDataset.scores |  probability of a positive event.\n",
    "| Labels  | BinaryLabelDataset.labels  | record of a positive event. Can be 1 or 0. \n",
    "| Weights  | BinaryLabelDataset.instance_weights  | when using Reweighing, instance weights will be placed here.\n",
    "\n",
    "When we run an inference over the dataset, we replace both the scores and the labels with the model's prediction over the dataset's features.\n",
    "\n",
    "When we measure bias metrics with ClassificationMetric, we generally need to pass in two BinaryLabelDataset objects. The first object will be the original dataset, the second will be the dataset which has had some inference run on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure key bias metrics using the original model to predict on the test set\n",
    "\n",
    " - Here we want to measure our key bias metrics (equal opportunity difference and disparate impact), as well as measure accuracy and precision in general, with AIF-360.\n",
    " - This serves both to confirm the manual measurements you already made and to serve as a benchmark for each debiasing method we will try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred = aif360_biased_dataset_test.copy()\n",
    "dataset_pred.labels = biased_rf.predict(aif360_biased_dataset_test.features)\n",
    "dataset_pred.scores = biased_rf.predict_proba(aif360_biased_dataset_test.features)[:,1]\n",
    "\n",
    "unprivileged_groups = [{'gender':1.0}] #women\n",
    "privileged_groups = [{'gender':2.0}] #men\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "        aif360_biased_dataset_test, dataset_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "\n",
    "results['original_model'] = {'accuracy':metric.accuracy(),\n",
    "                               'precision':metric.precision(),\n",
    "                               'eq_diff': metric.equal_opportunity_difference(),\n",
    "'disparate_impact':metric.disparate_impact()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data using Reweighing\n",
    "\n",
    "Reweighing works by assigning weights to each row in the train dataset, such that the proportion of (group, outcome) is equal. A model can be then trained on the reweighed train dataset and tested on the test dataset without any reweighing. The steps are as follows:\n",
    "\n",
    " - Initialize reweighing object\n",
    " - Using fit_transform, create a reweighed train dataset from aif360_biased_dataset_train\n",
    " - Create a new model (eg RF) and train it on the reweighed train dataset\n",
    " - Use the new model to predict on the test dataset\n",
    " - Using ClassificationMetric, evalutae performance on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups) #initialize reweighing object\n",
    "reweighed_dataset = RW.fit_transform(aif360_biased_dataset_train) #create reweighed train dataset\n",
    "\n",
    "#train new model on reweighed dataset\n",
    "reweighed_rf = RandomForestClassifier(random_state=42) \n",
    "reweighed_rf.fit(reweighed_dataset.features, reweighed_dataset.labels.ravel(), sample_weight=reweighed_dataset.instance_weights)\n",
    "\n",
    "#infer on the test dataset using new model\n",
    "reweighing_test = aif360_biased_dataset_test.copy()\n",
    "reweighing_test.scores = reweighed_rf.predict_proba(aif360_biased_dataset_test.features)[:,1]\n",
    "reweighing_test.labels = reweighed_rf.predict(aif360_biased_dataset_test.features) \n",
    "\n",
    "metric = ClassificationMetric(\n",
    "        aif360_biased_dataset_test, reweighing_test,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "\n",
    "results['reweigher'] = {'accuracy':metric.accuracy(),\n",
    "                        'precision':metric.precision(),\n",
    "                        'eq_diff': metric.equal_opportunity_difference(),\n",
    "                        'disparate_impact':metric.disparate_impact()} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data using Disparate Impact Remover\n",
    "\n",
    "Disparate Impact Remover works by dividing the training set according to protected class and ensuring that the features in all divisions have similar distributions. A model can be then trained on the altered train dataset and tested on the unchanged test dataset. The steps are as follows:\n",
    "\n",
    " - Initialize Disparate Impact Remover object (you need to pass in the 'sensitive_attribute' and repair level)\n",
    " - Using fit_transform, create an altered train dataset from aif360_biased_dataset_train\n",
    " - Create a new model (eg RF) and train it on the reweighed train dataset\n",
    " - Use the new model to predict on the test dataset\n",
    " - Using ClassificationMetric, evalutae performance on the test dataset\n",
    " \n",
    "This pipeline is very similar to the Reweighing pipeline. Use the Reweighing pipeline code as a guide to implement this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## In-processing debiasing using Prejudice Remover\n",
    "\n",
    "Prejudice Remover trains a model with a regularization term to penalize discrimination along protected-class lines. For this method, the original model is not necessary. The steps are as follows:\n",
    "\n",
    " - Initialize PrejudiceRemover object\n",
    " - Train the PrejudiceRemover on the train dataset\n",
    " - Use the PrejudiceRemover to predict on the test dataset\n",
    " - Using ClassificationMetric, evaluatee performance on the test dataset\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing debiasing using Calibrated Equal Odds Postprocessing\n",
    "\n",
    "Calibrated Equal Odds Postprocessing adds an additional objective for regions of the model's prediction which carry high differences in odds between the protected and unprotected class members. The steps are as fllows:\n",
    "\n",
    " - Use the original model to predict on both the train and test sets\n",
    " - Initialize the CalibratedEqOddsPostprocessing object \n",
    " - Train the postprocessor on the model's predictions for the train dataset\n",
    " - Run the postprocessor on the model's predictions for the test dataset\n",
    " - Using ClassificationMetric, evaluate the performance of the postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = CalibratedEqOddsPostprocessing(unprivileged_groups, privileged_groups)\n",
    "dataset_pred_train = aif360_biased_dataset_train.copy()\n",
    "dataset_pred_test = aif360_biased_dataset_test.copy()\n",
    "dataset_pred_train.labels = biased_rf.predict(aif360_biased_dataset_train.features)\n",
    "dataset_pred_train.scores = biased_rf.predict_proba(aif360_biased_dataset_train.features)[:,1]\n",
    "dataset_pred_test.labels = biased_rf.predict(aif360_biased_dataset_test.features)\n",
    "dataset_pred_test.scores = biased_rf.predict_proba(aif360_biased_dataset_test.features)[:,1]\n",
    "\n",
    "postprocessor.fit(aif360_biased_dataset_train, dataset_pred_train)\n",
    "\n",
    "postprocessed = postprocessor.predict(dataset_pred_test)\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "        aif360_biased_dataset_test, postprocessed,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "\n",
    "results['post-calibration'] = {'accuracy':metric.accuracy(),\n",
    "                               'precision':metric.precision(),\n",
    "                                'eq_diff': metric.equal_opportunity_difference(),\n",
    "                                'disparate_impact':metric.disparate_impact()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize your results\n",
    "Review the accuracy, precision, equal opportunity difference, and disparate impact for the original model and each debiasing method. Does debiasing make a difference? Is there a large tradeoff between accuracy/precision and the debiasing process? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>eq_diff</th>\n",
       "      <th>disparate_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original_model</th>\n",
       "      <td>0.740994</td>\n",
       "      <td>0.722542</td>\n",
       "      <td>-0.122617</td>\n",
       "      <td>0.655888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reweigher</th>\n",
       "      <td>0.740928</td>\n",
       "      <td>0.724929</td>\n",
       "      <td>-0.105570</td>\n",
       "      <td>0.673714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI</th>\n",
       "      <td>0.744537</td>\n",
       "      <td>0.716520</td>\n",
       "      <td>-0.048534</td>\n",
       "      <td>0.756556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post-calibration</th>\n",
       "      <td>0.738106</td>\n",
       "      <td>0.716958</td>\n",
       "      <td>-0.135273</td>\n",
       "      <td>0.639007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  accuracy  precision   eq_diff  disparate_impact\n",
       "original_model    0.740994   0.722542 -0.122617          0.655888\n",
       "reweigher         0.740928   0.724929 -0.105570          0.673714\n",
       "DI                0.744537   0.716520 -0.048534          0.756556\n",
       "post-calibration  0.738106   0.716958 -0.135273          0.639007"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of both Reweighing and Disparate Impact Removers is a new classifier object. These classifiers can be fine-tuned further by selecting a threshold.\n",
    "\n",
    "The default threshold for scikit-learn classifiers is 0.5 . The classifier's binary output represents an expected probability either below or above/equal this theshold. We can manipulate the threshold if we see that a different threshold shows improved results.\n",
    "\n",
    "When debiasing, threshold selection is especially important, because it gives data scientists a sense of how much model performance could be sacrificed to reach a certain bias measure (eg >0.8 Disparate Impact). If the sacrifice to model performance is low, an alternate threshold with better bias metrics could be chosen. \n",
    "\n",
    "Below you are provided some sample code which evaluates each percentage threshold from 0% to 100% for the original biased model and Reweighing model. You are asked to do the following:\n",
    "\n",
    " - Run the code to evaluate each percentage threshold\n",
    " - Add additional code to evaluate the percentage thresholds for the Reweighing model\n",
    " - Observe the shifts in accuracy, precision, disparate impact and average odds difference across each threshold\n",
    " - Is it possible to say which model, provides the best performance and at which threshold? Would you recommend a non-default threshold for your preferred model? Refer to the resource on impossibility for an explanation of why one threshold will not maximize all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_original</th>\n",
       "      <th>average_odds_difference_original</th>\n",
       "      <th>disparate_impact_original</th>\n",
       "      <th>accuracy_reweighed</th>\n",
       "      <th>average_odds_difference_reweighed</th>\n",
       "      <th>disparate_impact_reweighed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshhold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.426275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.426275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.427784</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>0.997064</td>\n",
       "      <td>0.428375</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>0.996851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02</th>\n",
       "      <td>0.432509</td>\n",
       "      <td>-0.008438</td>\n",
       "      <td>0.989702</td>\n",
       "      <td>0.433821</td>\n",
       "      <td>-0.009890</td>\n",
       "      <td>0.987936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03</th>\n",
       "      <td>0.438808</td>\n",
       "      <td>-0.016678</td>\n",
       "      <td>0.979646</td>\n",
       "      <td>0.439858</td>\n",
       "      <td>-0.016511</td>\n",
       "      <td>0.979523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.04</th>\n",
       "      <td>0.448520</td>\n",
       "      <td>-0.027585</td>\n",
       "      <td>0.965721</td>\n",
       "      <td>0.446224</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>0.970664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.456920</td>\n",
       "      <td>-0.036411</td>\n",
       "      <td>0.954564</td>\n",
       "      <td>0.455542</td>\n",
       "      <td>-0.030546</td>\n",
       "      <td>0.960737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>0.465713</td>\n",
       "      <td>-0.045847</td>\n",
       "      <td>0.942364</td>\n",
       "      <td>0.466107</td>\n",
       "      <td>-0.036755</td>\n",
       "      <td>0.951320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07</th>\n",
       "      <td>0.477131</td>\n",
       "      <td>-0.055026</td>\n",
       "      <td>0.929514</td>\n",
       "      <td>0.476869</td>\n",
       "      <td>-0.046570</td>\n",
       "      <td>0.938014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08</th>\n",
       "      <td>0.486909</td>\n",
       "      <td>-0.062321</td>\n",
       "      <td>0.919056</td>\n",
       "      <td>0.486777</td>\n",
       "      <td>-0.054464</td>\n",
       "      <td>0.926980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09</th>\n",
       "      <td>0.498917</td>\n",
       "      <td>-0.071134</td>\n",
       "      <td>0.906254</td>\n",
       "      <td>0.497867</td>\n",
       "      <td>-0.061879</td>\n",
       "      <td>0.915891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.510073</td>\n",
       "      <td>-0.078793</td>\n",
       "      <td>0.894414</td>\n",
       "      <td>0.507448</td>\n",
       "      <td>-0.067720</td>\n",
       "      <td>0.906465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>0.522082</td>\n",
       "      <td>-0.087860</td>\n",
       "      <td>0.880788</td>\n",
       "      <td>0.517751</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>0.895030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.12</th>\n",
       "      <td>0.531793</td>\n",
       "      <td>-0.095964</td>\n",
       "      <td>0.868696</td>\n",
       "      <td>0.528119</td>\n",
       "      <td>-0.081945</td>\n",
       "      <td>0.884159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.13</th>\n",
       "      <td>0.541768</td>\n",
       "      <td>-0.101729</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>0.538552</td>\n",
       "      <td>-0.088995</td>\n",
       "      <td>0.872921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.14</th>\n",
       "      <td>0.553055</td>\n",
       "      <td>-0.108109</td>\n",
       "      <td>0.847119</td>\n",
       "      <td>0.549446</td>\n",
       "      <td>-0.094280</td>\n",
       "      <td>0.862972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>0.563226</td>\n",
       "      <td>-0.114091</td>\n",
       "      <td>0.836028</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>-0.098141</td>\n",
       "      <td>0.854579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>0.575103</td>\n",
       "      <td>-0.117951</td>\n",
       "      <td>0.826507</td>\n",
       "      <td>0.570313</td>\n",
       "      <td>-0.102429</td>\n",
       "      <td>0.845079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.17</th>\n",
       "      <td>0.585143</td>\n",
       "      <td>-0.122259</td>\n",
       "      <td>0.817012</td>\n",
       "      <td>0.580091</td>\n",
       "      <td>-0.110558</td>\n",
       "      <td>0.831615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.18</th>\n",
       "      <td>0.594133</td>\n",
       "      <td>-0.128071</td>\n",
       "      <td>0.805595</td>\n",
       "      <td>0.591115</td>\n",
       "      <td>-0.112562</td>\n",
       "      <td>0.823469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.19</th>\n",
       "      <td>0.603189</td>\n",
       "      <td>-0.133218</td>\n",
       "      <td>0.794974</td>\n",
       "      <td>0.600892</td>\n",
       "      <td>-0.114883</td>\n",
       "      <td>0.815251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>0.610932</td>\n",
       "      <td>-0.133538</td>\n",
       "      <td>0.789196</td>\n",
       "      <td>0.611786</td>\n",
       "      <td>-0.117814</td>\n",
       "      <td>0.805788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>0.620973</td>\n",
       "      <td>-0.135261</td>\n",
       "      <td>0.781379</td>\n",
       "      <td>0.620710</td>\n",
       "      <td>-0.121071</td>\n",
       "      <td>0.796953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>0.627994</td>\n",
       "      <td>-0.139865</td>\n",
       "      <td>0.771065</td>\n",
       "      <td>0.627928</td>\n",
       "      <td>-0.122489</td>\n",
       "      <td>0.790525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.23</th>\n",
       "      <td>0.637443</td>\n",
       "      <td>-0.144288</td>\n",
       "      <td>0.759831</td>\n",
       "      <td>0.636525</td>\n",
       "      <td>-0.123108</td>\n",
       "      <td>0.783886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.24</th>\n",
       "      <td>0.644137</td>\n",
       "      <td>-0.148415</td>\n",
       "      <td>0.749376</td>\n",
       "      <td>0.645252</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>0.774932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.651093</td>\n",
       "      <td>-0.152766</td>\n",
       "      <td>0.738790</td>\n",
       "      <td>0.652011</td>\n",
       "      <td>-0.124078</td>\n",
       "      <td>0.770919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>0.657327</td>\n",
       "      <td>-0.155191</td>\n",
       "      <td>0.729712</td>\n",
       "      <td>0.659295</td>\n",
       "      <td>-0.123033</td>\n",
       "      <td>0.766197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.27</th>\n",
       "      <td>0.663823</td>\n",
       "      <td>-0.156521</td>\n",
       "      <td>0.722530</td>\n",
       "      <td>0.665660</td>\n",
       "      <td>-0.122055</td>\n",
       "      <td>0.761523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.28</th>\n",
       "      <td>0.669532</td>\n",
       "      <td>-0.155098</td>\n",
       "      <td>0.718113</td>\n",
       "      <td>0.672354</td>\n",
       "      <td>-0.125325</td>\n",
       "      <td>0.751629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.29</th>\n",
       "      <td>0.676094</td>\n",
       "      <td>-0.154340</td>\n",
       "      <td>0.712511</td>\n",
       "      <td>0.679047</td>\n",
       "      <td>-0.127222</td>\n",
       "      <td>0.743026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>0.683050</td>\n",
       "      <td>-0.151284</td>\n",
       "      <td>0.709811</td>\n",
       "      <td>0.684559</td>\n",
       "      <td>-0.129435</td>\n",
       "      <td>0.734374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>0.688497</td>\n",
       "      <td>-0.149907</td>\n",
       "      <td>0.704958</td>\n",
       "      <td>0.688103</td>\n",
       "      <td>-0.127660</td>\n",
       "      <td>0.731877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.32</th>\n",
       "      <td>0.692762</td>\n",
       "      <td>-0.148156</td>\n",
       "      <td>0.701158</td>\n",
       "      <td>0.691778</td>\n",
       "      <td>-0.127649</td>\n",
       "      <td>0.726357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.33</th>\n",
       "      <td>0.697027</td>\n",
       "      <td>-0.143998</td>\n",
       "      <td>0.700212</td>\n",
       "      <td>0.695912</td>\n",
       "      <td>-0.128171</td>\n",
       "      <td>0.720707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.34</th>\n",
       "      <td>0.700440</td>\n",
       "      <td>-0.143229</td>\n",
       "      <td>0.695001</td>\n",
       "      <td>0.700899</td>\n",
       "      <td>-0.128033</td>\n",
       "      <td>0.714716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>0.708117</td>\n",
       "      <td>-0.136333</td>\n",
       "      <td>0.692059</td>\n",
       "      <td>0.708249</td>\n",
       "      <td>-0.126844</td>\n",
       "      <td>0.704834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>0.708117</td>\n",
       "      <td>-0.136333</td>\n",
       "      <td>0.692059</td>\n",
       "      <td>0.708249</td>\n",
       "      <td>-0.126844</td>\n",
       "      <td>0.704834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.37</th>\n",
       "      <td>0.712580</td>\n",
       "      <td>-0.132819</td>\n",
       "      <td>0.690221</td>\n",
       "      <td>0.711989</td>\n",
       "      <td>-0.125461</td>\n",
       "      <td>0.700890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.38</th>\n",
       "      <td>0.716254</td>\n",
       "      <td>-0.131779</td>\n",
       "      <td>0.685736</td>\n",
       "      <td>0.715139</td>\n",
       "      <td>-0.123884</td>\n",
       "      <td>0.697640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.39</th>\n",
       "      <td>0.718289</td>\n",
       "      <td>-0.131044</td>\n",
       "      <td>0.681262</td>\n",
       "      <td>0.720782</td>\n",
       "      <td>-0.120164</td>\n",
       "      <td>0.696192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>0.721701</td>\n",
       "      <td>-0.129395</td>\n",
       "      <td>0.677168</td>\n",
       "      <td>0.723341</td>\n",
       "      <td>-0.116026</td>\n",
       "      <td>0.697109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>0.726294</td>\n",
       "      <td>-0.127795</td>\n",
       "      <td>0.668924</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>-0.115021</td>\n",
       "      <td>0.684819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.42</th>\n",
       "      <td>0.726294</td>\n",
       "      <td>-0.127795</td>\n",
       "      <td>0.668924</td>\n",
       "      <td>0.730822</td>\n",
       "      <td>-0.115021</td>\n",
       "      <td>0.684819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.43</th>\n",
       "      <td>0.727869</td>\n",
       "      <td>-0.126939</td>\n",
       "      <td>0.665586</td>\n",
       "      <td>0.731938</td>\n",
       "      <td>-0.110516</td>\n",
       "      <td>0.686783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.44</th>\n",
       "      <td>0.729444</td>\n",
       "      <td>-0.122937</td>\n",
       "      <td>0.665725</td>\n",
       "      <td>0.733185</td>\n",
       "      <td>-0.109448</td>\n",
       "      <td>0.682618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>0.731413</td>\n",
       "      <td>-0.118515</td>\n",
       "      <td>0.666687</td>\n",
       "      <td>0.734759</td>\n",
       "      <td>-0.110139</td>\n",
       "      <td>0.676390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.46</th>\n",
       "      <td>0.732397</td>\n",
       "      <td>-0.117890</td>\n",
       "      <td>0.662470</td>\n",
       "      <td>0.736072</td>\n",
       "      <td>-0.107526</td>\n",
       "      <td>0.674196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.47</th>\n",
       "      <td>0.735284</td>\n",
       "      <td>-0.113969</td>\n",
       "      <td>0.658353</td>\n",
       "      <td>0.738434</td>\n",
       "      <td>-0.105332</td>\n",
       "      <td>0.666902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.48</th>\n",
       "      <td>0.735284</td>\n",
       "      <td>-0.113969</td>\n",
       "      <td>0.658353</td>\n",
       "      <td>0.738434</td>\n",
       "      <td>-0.105332</td>\n",
       "      <td>0.666902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.49</th>\n",
       "      <td>0.737253</td>\n",
       "      <td>-0.111082</td>\n",
       "      <td>0.657035</td>\n",
       "      <td>0.740075</td>\n",
       "      <td>-0.100468</td>\n",
       "      <td>0.670067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.738894</td>\n",
       "      <td>-0.109053</td>\n",
       "      <td>0.654982</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>-0.095555</td>\n",
       "      <td>0.672876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.51</th>\n",
       "      <td>0.740994</td>\n",
       "      <td>-0.105201</td>\n",
       "      <td>0.655888</td>\n",
       "      <td>0.740928</td>\n",
       "      <td>-0.092591</td>\n",
       "      <td>0.673714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.52</th>\n",
       "      <td>0.741518</td>\n",
       "      <td>-0.101692</td>\n",
       "      <td>0.655340</td>\n",
       "      <td>0.740403</td>\n",
       "      <td>-0.089287</td>\n",
       "      <td>0.674581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.53</th>\n",
       "      <td>0.742109</td>\n",
       "      <td>-0.099696</td>\n",
       "      <td>0.653693</td>\n",
       "      <td>0.740534</td>\n",
       "      <td>-0.088367</td>\n",
       "      <td>0.671517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.54</th>\n",
       "      <td>0.743093</td>\n",
       "      <td>-0.095845</td>\n",
       "      <td>0.654878</td>\n",
       "      <td>0.739812</td>\n",
       "      <td>-0.088719</td>\n",
       "      <td>0.667631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.743159</td>\n",
       "      <td>-0.093183</td>\n",
       "      <td>0.654112</td>\n",
       "      <td>0.738303</td>\n",
       "      <td>-0.084770</td>\n",
       "      <td>0.669671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.56</th>\n",
       "      <td>0.742175</td>\n",
       "      <td>-0.089990</td>\n",
       "      <td>0.654266</td>\n",
       "      <td>0.738434</td>\n",
       "      <td>-0.084088</td>\n",
       "      <td>0.665817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.57</th>\n",
       "      <td>0.738828</td>\n",
       "      <td>-0.090356</td>\n",
       "      <td>0.645010</td>\n",
       "      <td>0.737056</td>\n",
       "      <td>-0.083006</td>\n",
       "      <td>0.659604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.58</th>\n",
       "      <td>0.738828</td>\n",
       "      <td>-0.090356</td>\n",
       "      <td>0.645010</td>\n",
       "      <td>0.737056</td>\n",
       "      <td>-0.083006</td>\n",
       "      <td>0.659604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.59</th>\n",
       "      <td>0.736991</td>\n",
       "      <td>-0.089476</td>\n",
       "      <td>0.641470</td>\n",
       "      <td>0.736466</td>\n",
       "      <td>-0.080362</td>\n",
       "      <td>0.659463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.736138</td>\n",
       "      <td>-0.086853</td>\n",
       "      <td>0.641455</td>\n",
       "      <td>0.735022</td>\n",
       "      <td>-0.080110</td>\n",
       "      <td>0.656281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.61</th>\n",
       "      <td>0.734300</td>\n",
       "      <td>-0.087904</td>\n",
       "      <td>0.635040</td>\n",
       "      <td>0.734235</td>\n",
       "      <td>-0.078160</td>\n",
       "      <td>0.654539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.62</th>\n",
       "      <td>0.734563</td>\n",
       "      <td>-0.088597</td>\n",
       "      <td>0.627851</td>\n",
       "      <td>0.730888</td>\n",
       "      <td>-0.077225</td>\n",
       "      <td>0.652485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.63</th>\n",
       "      <td>0.731610</td>\n",
       "      <td>-0.086417</td>\n",
       "      <td>0.626783</td>\n",
       "      <td>0.729247</td>\n",
       "      <td>-0.076767</td>\n",
       "      <td>0.649202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.64</th>\n",
       "      <td>0.728985</td>\n",
       "      <td>-0.086254</td>\n",
       "      <td>0.623277</td>\n",
       "      <td>0.728263</td>\n",
       "      <td>-0.076840</td>\n",
       "      <td>0.642598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>0.727738</td>\n",
       "      <td>-0.088627</td>\n",
       "      <td>0.613292</td>\n",
       "      <td>0.727082</td>\n",
       "      <td>-0.076093</td>\n",
       "      <td>0.639253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.66</th>\n",
       "      <td>0.724457</td>\n",
       "      <td>-0.087539</td>\n",
       "      <td>0.608941</td>\n",
       "      <td>0.725507</td>\n",
       "      <td>-0.076309</td>\n",
       "      <td>0.633971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.67</th>\n",
       "      <td>0.720848</td>\n",
       "      <td>-0.087512</td>\n",
       "      <td>0.603467</td>\n",
       "      <td>0.722554</td>\n",
       "      <td>-0.076357</td>\n",
       "      <td>0.627017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.68</th>\n",
       "      <td>0.717829</td>\n",
       "      <td>-0.090512</td>\n",
       "      <td>0.591288</td>\n",
       "      <td>0.718092</td>\n",
       "      <td>-0.076540</td>\n",
       "      <td>0.622234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.69</th>\n",
       "      <td>0.708774</td>\n",
       "      <td>-0.097255</td>\n",
       "      <td>0.561593</td>\n",
       "      <td>0.711530</td>\n",
       "      <td>-0.078889</td>\n",
       "      <td>0.604104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.703983</td>\n",
       "      <td>-0.099569</td>\n",
       "      <td>0.548719</td>\n",
       "      <td>0.707724</td>\n",
       "      <td>-0.080537</td>\n",
       "      <td>0.592473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.71</th>\n",
       "      <td>0.703983</td>\n",
       "      <td>-0.099569</td>\n",
       "      <td>0.548719</td>\n",
       "      <td>0.707724</td>\n",
       "      <td>-0.080537</td>\n",
       "      <td>0.592473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.72</th>\n",
       "      <td>0.700243</td>\n",
       "      <td>-0.101286</td>\n",
       "      <td>0.534590</td>\n",
       "      <td>0.704180</td>\n",
       "      <td>-0.079774</td>\n",
       "      <td>0.584745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.73</th>\n",
       "      <td>0.696240</td>\n",
       "      <td>-0.103528</td>\n",
       "      <td>0.518776</td>\n",
       "      <td>0.700965</td>\n",
       "      <td>-0.079952</td>\n",
       "      <td>0.572537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.74</th>\n",
       "      <td>0.691778</td>\n",
       "      <td>-0.105893</td>\n",
       "      <td>0.499182</td>\n",
       "      <td>0.695912</td>\n",
       "      <td>-0.083436</td>\n",
       "      <td>0.554003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.687184</td>\n",
       "      <td>-0.109763</td>\n",
       "      <td>0.475973</td>\n",
       "      <td>0.691384</td>\n",
       "      <td>-0.084030</td>\n",
       "      <td>0.541849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.76</th>\n",
       "      <td>0.681672</td>\n",
       "      <td>-0.109917</td>\n",
       "      <td>0.460357</td>\n",
       "      <td>0.685150</td>\n",
       "      <td>-0.085016</td>\n",
       "      <td>0.526120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.77</th>\n",
       "      <td>0.677341</td>\n",
       "      <td>-0.111408</td>\n",
       "      <td>0.439628</td>\n",
       "      <td>0.679178</td>\n",
       "      <td>-0.089291</td>\n",
       "      <td>0.498586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.78</th>\n",
       "      <td>0.671698</td>\n",
       "      <td>-0.110087</td>\n",
       "      <td>0.424173</td>\n",
       "      <td>0.672026</td>\n",
       "      <td>-0.089759</td>\n",
       "      <td>0.479979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.79</th>\n",
       "      <td>0.665989</td>\n",
       "      <td>-0.108135</td>\n",
       "      <td>0.409287</td>\n",
       "      <td>0.665857</td>\n",
       "      <td>-0.087944</td>\n",
       "      <td>0.467473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>0.659033</td>\n",
       "      <td>-0.105320</td>\n",
       "      <td>0.393704</td>\n",
       "      <td>0.658508</td>\n",
       "      <td>-0.083551</td>\n",
       "      <td>0.457402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.81</th>\n",
       "      <td>0.651880</td>\n",
       "      <td>-0.104386</td>\n",
       "      <td>0.369052</td>\n",
       "      <td>0.650568</td>\n",
       "      <td>-0.080107</td>\n",
       "      <td>0.441449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.82</th>\n",
       "      <td>0.638493</td>\n",
       "      <td>-0.092582</td>\n",
       "      <td>0.339587</td>\n",
       "      <td>0.636393</td>\n",
       "      <td>-0.079043</td>\n",
       "      <td>0.384091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.83</th>\n",
       "      <td>0.631997</td>\n",
       "      <td>-0.090436</td>\n",
       "      <td>0.309081</td>\n",
       "      <td>0.631406</td>\n",
       "      <td>-0.074943</td>\n",
       "      <td>0.367712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.84</th>\n",
       "      <td>0.631997</td>\n",
       "      <td>-0.090436</td>\n",
       "      <td>0.309081</td>\n",
       "      <td>0.631406</td>\n",
       "      <td>-0.074943</td>\n",
       "      <td>0.367712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>0.625632</td>\n",
       "      <td>-0.082337</td>\n",
       "      <td>0.297568</td>\n",
       "      <td>0.624450</td>\n",
       "      <td>-0.067766</td>\n",
       "      <td>0.360614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.86</th>\n",
       "      <td>0.619004</td>\n",
       "      <td>-0.073421</td>\n",
       "      <td>0.292941</td>\n",
       "      <td>0.618348</td>\n",
       "      <td>-0.062855</td>\n",
       "      <td>0.338742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.87</th>\n",
       "      <td>0.613164</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.255632</td>\n",
       "      <td>0.612311</td>\n",
       "      <td>-0.054873</td>\n",
       "      <td>0.330486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.88</th>\n",
       "      <td>0.606405</td>\n",
       "      <td>-0.061685</td>\n",
       "      <td>0.237782</td>\n",
       "      <td>0.604961</td>\n",
       "      <td>-0.047961</td>\n",
       "      <td>0.309927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.89</th>\n",
       "      <td>0.600958</td>\n",
       "      <td>-0.053050</td>\n",
       "      <td>0.219687</td>\n",
       "      <td>0.598333</td>\n",
       "      <td>-0.041254</td>\n",
       "      <td>0.282704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.595577</td>\n",
       "      <td>-0.046671</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.593609</td>\n",
       "      <td>-0.033653</td>\n",
       "      <td>0.275236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.91</th>\n",
       "      <td>0.590918</td>\n",
       "      <td>-0.038882</td>\n",
       "      <td>0.148491</td>\n",
       "      <td>0.589343</td>\n",
       "      <td>-0.030665</td>\n",
       "      <td>0.213985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.92</th>\n",
       "      <td>0.586653</td>\n",
       "      <td>-0.029221</td>\n",
       "      <td>0.141224</td>\n",
       "      <td>0.585537</td>\n",
       "      <td>-0.023838</td>\n",
       "      <td>0.200777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.93</th>\n",
       "      <td>0.583437</td>\n",
       "      <td>-0.023555</td>\n",
       "      <td>0.111751</td>\n",
       "      <td>0.582781</td>\n",
       "      <td>-0.020449</td>\n",
       "      <td>0.140985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.94</th>\n",
       "      <td>0.578384</td>\n",
       "      <td>-0.011085</td>\n",
       "      <td>0.095684</td>\n",
       "      <td>0.577203</td>\n",
       "      <td>-0.009733</td>\n",
       "      <td>0.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>0.576088</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>0.047336</td>\n",
       "      <td>0.575825</td>\n",
       "      <td>-0.006227</td>\n",
       "      <td>0.064618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.576088</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>0.047336</td>\n",
       "      <td>0.575825</td>\n",
       "      <td>-0.006227</td>\n",
       "      <td>0.064618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.97</th>\n",
       "      <td>0.575038</td>\n",
       "      <td>-0.003369</td>\n",
       "      <td>0.088499</td>\n",
       "      <td>0.574644</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>0.067849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.98</th>\n",
       "      <td>0.574185</td>\n",
       "      <td>-0.000814</td>\n",
       "      <td>0.113081</td>\n",
       "      <td>0.574447</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>0.056541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.573988</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573791</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.573791</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573791</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy_original  average_odds_difference_original  \\\n",
       "threshhold                                                        \n",
       "0.00                 0.426275                          0.000000   \n",
       "0.01                 0.427784                         -0.002530   \n",
       "0.02                 0.432509                         -0.008438   \n",
       "0.03                 0.438808                         -0.016678   \n",
       "0.04                 0.448520                         -0.027585   \n",
       "0.05                 0.456920                         -0.036411   \n",
       "0.06                 0.465713                         -0.045847   \n",
       "0.07                 0.477131                         -0.055026   \n",
       "0.08                 0.486909                         -0.062321   \n",
       "0.09                 0.498917                         -0.071134   \n",
       "0.10                 0.510073                         -0.078793   \n",
       "0.11                 0.522082                         -0.087860   \n",
       "0.12                 0.531793                         -0.095964   \n",
       "0.13                 0.541768                         -0.101729   \n",
       "0.14                 0.553055                         -0.108109   \n",
       "0.15                 0.563226                         -0.114091   \n",
       "0.16                 0.575103                         -0.117951   \n",
       "0.17                 0.585143                         -0.122259   \n",
       "0.18                 0.594133                         -0.128071   \n",
       "0.19                 0.603189                         -0.133218   \n",
       "0.20                 0.610932                         -0.133538   \n",
       "0.21                 0.620973                         -0.135261   \n",
       "0.22                 0.627994                         -0.139865   \n",
       "0.23                 0.637443                         -0.144288   \n",
       "0.24                 0.644137                         -0.148415   \n",
       "0.25                 0.651093                         -0.152766   \n",
       "0.26                 0.657327                         -0.155191   \n",
       "0.27                 0.663823                         -0.156521   \n",
       "0.28                 0.669532                         -0.155098   \n",
       "0.29                 0.676094                         -0.154340   \n",
       "0.30                 0.683050                         -0.151284   \n",
       "0.31                 0.688497                         -0.149907   \n",
       "0.32                 0.692762                         -0.148156   \n",
       "0.33                 0.697027                         -0.143998   \n",
       "0.34                 0.700440                         -0.143229   \n",
       "0.35                 0.708117                         -0.136333   \n",
       "0.36                 0.708117                         -0.136333   \n",
       "0.37                 0.712580                         -0.132819   \n",
       "0.38                 0.716254                         -0.131779   \n",
       "0.39                 0.718289                         -0.131044   \n",
       "0.40                 0.721701                         -0.129395   \n",
       "0.41                 0.726294                         -0.127795   \n",
       "0.42                 0.726294                         -0.127795   \n",
       "0.43                 0.727869                         -0.126939   \n",
       "0.44                 0.729444                         -0.122937   \n",
       "0.45                 0.731413                         -0.118515   \n",
       "0.46                 0.732397                         -0.117890   \n",
       "0.47                 0.735284                         -0.113969   \n",
       "0.48                 0.735284                         -0.113969   \n",
       "0.49                 0.737253                         -0.111082   \n",
       "0.50                 0.738894                         -0.109053   \n",
       "0.51                 0.740994                         -0.105201   \n",
       "0.52                 0.741518                         -0.101692   \n",
       "0.53                 0.742109                         -0.099696   \n",
       "0.54                 0.743093                         -0.095845   \n",
       "0.55                 0.743159                         -0.093183   \n",
       "0.56                 0.742175                         -0.089990   \n",
       "0.57                 0.738828                         -0.090356   \n",
       "0.58                 0.738828                         -0.090356   \n",
       "0.59                 0.736991                         -0.089476   \n",
       "0.60                 0.736138                         -0.086853   \n",
       "0.61                 0.734300                         -0.087904   \n",
       "0.62                 0.734563                         -0.088597   \n",
       "0.63                 0.731610                         -0.086417   \n",
       "0.64                 0.728985                         -0.086254   \n",
       "0.65                 0.727738                         -0.088627   \n",
       "0.66                 0.724457                         -0.087539   \n",
       "0.67                 0.720848                         -0.087512   \n",
       "0.68                 0.717829                         -0.090512   \n",
       "0.69                 0.708774                         -0.097255   \n",
       "0.70                 0.703983                         -0.099569   \n",
       "0.71                 0.703983                         -0.099569   \n",
       "0.72                 0.700243                         -0.101286   \n",
       "0.73                 0.696240                         -0.103528   \n",
       "0.74                 0.691778                         -0.105893   \n",
       "0.75                 0.687184                         -0.109763   \n",
       "0.76                 0.681672                         -0.109917   \n",
       "0.77                 0.677341                         -0.111408   \n",
       "0.78                 0.671698                         -0.110087   \n",
       "0.79                 0.665989                         -0.108135   \n",
       "0.80                 0.659033                         -0.105320   \n",
       "0.81                 0.651880                         -0.104386   \n",
       "0.82                 0.638493                         -0.092582   \n",
       "0.83                 0.631997                         -0.090436   \n",
       "0.84                 0.631997                         -0.090436   \n",
       "0.85                 0.625632                         -0.082337   \n",
       "0.86                 0.619004                         -0.073421   \n",
       "0.87                 0.613164                         -0.069824   \n",
       "0.88                 0.606405                         -0.061685   \n",
       "0.89                 0.600958                         -0.053050   \n",
       "0.90                 0.595577                         -0.046671   \n",
       "0.91                 0.590918                         -0.038882   \n",
       "0.92                 0.586653                         -0.029221   \n",
       "0.93                 0.583437                         -0.023555   \n",
       "0.94                 0.578384                         -0.011085   \n",
       "0.95                 0.576088                         -0.006584   \n",
       "0.96                 0.576088                         -0.006584   \n",
       "0.97                 0.575038                         -0.003369   \n",
       "0.98                 0.574185                         -0.000814   \n",
       "0.99                 0.573988                         -0.000641   \n",
       "1.00                 0.573791                         -0.000160   \n",
       "\n",
       "            disparate_impact_original  accuracy_reweighed  \\\n",
       "threshhold                                                  \n",
       "0.00                         1.000000            0.426275   \n",
       "0.01                         0.997064            0.428375   \n",
       "0.02                         0.989702            0.433821   \n",
       "0.03                         0.979646            0.439858   \n",
       "0.04                         0.965721            0.446224   \n",
       "0.05                         0.954564            0.455542   \n",
       "0.06                         0.942364            0.466107   \n",
       "0.07                         0.929514            0.476869   \n",
       "0.08                         0.919056            0.486777   \n",
       "0.09                         0.906254            0.497867   \n",
       "0.10                         0.894414            0.507448   \n",
       "0.11                         0.880788            0.517751   \n",
       "0.12                         0.868696            0.528119   \n",
       "0.13                         0.858603            0.538552   \n",
       "0.14                         0.847119            0.549446   \n",
       "0.15                         0.836028            0.559748   \n",
       "0.16                         0.826507            0.570313   \n",
       "0.17                         0.817012            0.580091   \n",
       "0.18                         0.805595            0.591115   \n",
       "0.19                         0.794974            0.600892   \n",
       "0.20                         0.789196            0.611786   \n",
       "0.21                         0.781379            0.620710   \n",
       "0.22                         0.771065            0.627928   \n",
       "0.23                         0.759831            0.636525   \n",
       "0.24                         0.749376            0.645252   \n",
       "0.25                         0.738790            0.652011   \n",
       "0.26                         0.729712            0.659295   \n",
       "0.27                         0.722530            0.665660   \n",
       "0.28                         0.718113            0.672354   \n",
       "0.29                         0.712511            0.679047   \n",
       "0.30                         0.709811            0.684559   \n",
       "0.31                         0.704958            0.688103   \n",
       "0.32                         0.701158            0.691778   \n",
       "0.33                         0.700212            0.695912   \n",
       "0.34                         0.695001            0.700899   \n",
       "0.35                         0.692059            0.708249   \n",
       "0.36                         0.692059            0.708249   \n",
       "0.37                         0.690221            0.711989   \n",
       "0.38                         0.685736            0.715139   \n",
       "0.39                         0.681262            0.720782   \n",
       "0.40                         0.677168            0.723341   \n",
       "0.41                         0.668924            0.730822   \n",
       "0.42                         0.668924            0.730822   \n",
       "0.43                         0.665586            0.731938   \n",
       "0.44                         0.665725            0.733185   \n",
       "0.45                         0.666687            0.734759   \n",
       "0.46                         0.662470            0.736072   \n",
       "0.47                         0.658353            0.738434   \n",
       "0.48                         0.658353            0.738434   \n",
       "0.49                         0.657035            0.740075   \n",
       "0.50                         0.654982            0.740665   \n",
       "0.51                         0.655888            0.740928   \n",
       "0.52                         0.655340            0.740403   \n",
       "0.53                         0.653693            0.740534   \n",
       "0.54                         0.654878            0.739812   \n",
       "0.55                         0.654112            0.738303   \n",
       "0.56                         0.654266            0.738434   \n",
       "0.57                         0.645010            0.737056   \n",
       "0.58                         0.645010            0.737056   \n",
       "0.59                         0.641470            0.736466   \n",
       "0.60                         0.641455            0.735022   \n",
       "0.61                         0.635040            0.734235   \n",
       "0.62                         0.627851            0.730888   \n",
       "0.63                         0.626783            0.729247   \n",
       "0.64                         0.623277            0.728263   \n",
       "0.65                         0.613292            0.727082   \n",
       "0.66                         0.608941            0.725507   \n",
       "0.67                         0.603467            0.722554   \n",
       "0.68                         0.591288            0.718092   \n",
       "0.69                         0.561593            0.711530   \n",
       "0.70                         0.548719            0.707724   \n",
       "0.71                         0.548719            0.707724   \n",
       "0.72                         0.534590            0.704180   \n",
       "0.73                         0.518776            0.700965   \n",
       "0.74                         0.499182            0.695912   \n",
       "0.75                         0.475973            0.691384   \n",
       "0.76                         0.460357            0.685150   \n",
       "0.77                         0.439628            0.679178   \n",
       "0.78                         0.424173            0.672026   \n",
       "0.79                         0.409287            0.665857   \n",
       "0.80                         0.393704            0.658508   \n",
       "0.81                         0.369052            0.650568   \n",
       "0.82                         0.339587            0.636393   \n",
       "0.83                         0.309081            0.631406   \n",
       "0.84                         0.309081            0.631406   \n",
       "0.85                         0.297568            0.624450   \n",
       "0.86                         0.292941            0.618348   \n",
       "0.87                         0.255632            0.612311   \n",
       "0.88                         0.237782            0.604961   \n",
       "0.89                         0.219687            0.598333   \n",
       "0.90                         0.179600            0.593609   \n",
       "0.91                         0.148491            0.589343   \n",
       "0.92                         0.141224            0.585537   \n",
       "0.93                         0.111751            0.582781   \n",
       "0.94                         0.095684            0.577203   \n",
       "0.95                         0.047336            0.575825   \n",
       "0.96                         0.047336            0.575825   \n",
       "0.97                         0.088499            0.574644   \n",
       "0.98                         0.113081            0.574447   \n",
       "0.99                         0.000000            0.573791   \n",
       "1.00                         0.000000            0.573791   \n",
       "\n",
       "            average_odds_difference_reweighed  disparate_impact_reweighed  \n",
       "threshhold                                                                 \n",
       "0.00                                 0.000000                    1.000000  \n",
       "0.01                                -0.002522                    0.996851  \n",
       "0.02                                -0.009890                    0.987936  \n",
       "0.03                                -0.016511                    0.979523  \n",
       "0.04                                -0.023409                    0.970664  \n",
       "0.05                                -0.030546                    0.960737  \n",
       "0.06                                -0.036755                    0.951320  \n",
       "0.07                                -0.046570                    0.938014  \n",
       "0.08                                -0.054464                    0.926980  \n",
       "0.09                                -0.061879                    0.915891  \n",
       "0.10                                -0.067720                    0.906465  \n",
       "0.11                                -0.075167                    0.895030  \n",
       "0.12                                -0.081945                    0.884159  \n",
       "0.13                                -0.088995                    0.872921  \n",
       "0.14                                -0.094280                    0.862972  \n",
       "0.15                                -0.098141                    0.854579  \n",
       "0.16                                -0.102429                    0.845079  \n",
       "0.17                                -0.110558                    0.831615  \n",
       "0.18                                -0.112562                    0.823469  \n",
       "0.19                                -0.114883                    0.815251  \n",
       "0.20                                -0.117814                    0.805788  \n",
       "0.21                                -0.121071                    0.796953  \n",
       "0.22                                -0.122489                    0.790525  \n",
       "0.23                                -0.123108                    0.783886  \n",
       "0.24                                -0.125601                    0.774932  \n",
       "0.25                                -0.124078                    0.770919  \n",
       "0.26                                -0.123033                    0.766197  \n",
       "0.27                                -0.122055                    0.761523  \n",
       "0.28                                -0.125325                    0.751629  \n",
       "0.29                                -0.127222                    0.743026  \n",
       "0.30                                -0.129435                    0.734374  \n",
       "0.31                                -0.127660                    0.731877  \n",
       "0.32                                -0.127649                    0.726357  \n",
       "0.33                                -0.128171                    0.720707  \n",
       "0.34                                -0.128033                    0.714716  \n",
       "0.35                                -0.126844                    0.704834  \n",
       "0.36                                -0.126844                    0.704834  \n",
       "0.37                                -0.125461                    0.700890  \n",
       "0.38                                -0.123884                    0.697640  \n",
       "0.39                                -0.120164                    0.696192  \n",
       "0.40                                -0.116026                    0.697109  \n",
       "0.41                                -0.115021                    0.684819  \n",
       "0.42                                -0.115021                    0.684819  \n",
       "0.43                                -0.110516                    0.686783  \n",
       "0.44                                -0.109448                    0.682618  \n",
       "0.45                                -0.110139                    0.676390  \n",
       "0.46                                -0.107526                    0.674196  \n",
       "0.47                                -0.105332                    0.666902  \n",
       "0.48                                -0.105332                    0.666902  \n",
       "0.49                                -0.100468                    0.670067  \n",
       "0.50                                -0.095555                    0.672876  \n",
       "0.51                                -0.092591                    0.673714  \n",
       "0.52                                -0.089287                    0.674581  \n",
       "0.53                                -0.088367                    0.671517  \n",
       "0.54                                -0.088719                    0.667631  \n",
       "0.55                                -0.084770                    0.669671  \n",
       "0.56                                -0.084088                    0.665817  \n",
       "0.57                                -0.083006                    0.659604  \n",
       "0.58                                -0.083006                    0.659604  \n",
       "0.59                                -0.080362                    0.659463  \n",
       "0.60                                -0.080110                    0.656281  \n",
       "0.61                                -0.078160                    0.654539  \n",
       "0.62                                -0.077225                    0.652485  \n",
       "0.63                                -0.076767                    0.649202  \n",
       "0.64                                -0.076840                    0.642598  \n",
       "0.65                                -0.076093                    0.639253  \n",
       "0.66                                -0.076309                    0.633971  \n",
       "0.67                                -0.076357                    0.627017  \n",
       "0.68                                -0.076540                    0.622234  \n",
       "0.69                                -0.078889                    0.604104  \n",
       "0.70                                -0.080537                    0.592473  \n",
       "0.71                                -0.080537                    0.592473  \n",
       "0.72                                -0.079774                    0.584745  \n",
       "0.73                                -0.079952                    0.572537  \n",
       "0.74                                -0.083436                    0.554003  \n",
       "0.75                                -0.084030                    0.541849  \n",
       "0.76                                -0.085016                    0.526120  \n",
       "0.77                                -0.089291                    0.498586  \n",
       "0.78                                -0.089759                    0.479979  \n",
       "0.79                                -0.087944                    0.467473  \n",
       "0.80                                -0.083551                    0.457402  \n",
       "0.81                                -0.080107                    0.441449  \n",
       "0.82                                -0.079043                    0.384091  \n",
       "0.83                                -0.074943                    0.367712  \n",
       "0.84                                -0.074943                    0.367712  \n",
       "0.85                                -0.067766                    0.360614  \n",
       "0.86                                -0.062855                    0.338742  \n",
       "0.87                                -0.054873                    0.330486  \n",
       "0.88                                -0.047961                    0.309927  \n",
       "0.89                                -0.041254                    0.282704  \n",
       "0.90                                -0.033653                    0.275236  \n",
       "0.91                                -0.030665                    0.213985  \n",
       "0.92                                -0.023838                    0.200777  \n",
       "0.93                                -0.020449                    0.140985  \n",
       "0.94                                -0.009733                    0.089800  \n",
       "0.95                                -0.006227                    0.064618  \n",
       "0.96                                -0.006227                    0.064618  \n",
       "0.97                                -0.002983                    0.067849  \n",
       "0.98                                -0.001780                    0.056541  \n",
       "0.99                                -0.000485                    0.000000  \n",
       "1.00                                -0.000160                    0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reports = []\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "for thresh in np.linspace(0,1,101):\n",
    "    #apply new threshold\n",
    "    dataset_pred.labels = dataset_pred.scores>=thresh \n",
    "    reweighing_test.labels = reweighing_test.scores>=thresh\n",
    "    \n",
    "    #get bias and acccuracy metrics\n",
    "    metric_original_rf = ClassificationMetric(\n",
    "        aif360_biased_dataset_test, dataset_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "    \n",
    "    metric_reweighed_rf = ClassificationMetric(\n",
    "        aif360_biased_dataset_test, reweighing_test,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "\n",
    "    #record metrics\n",
    "    reports.append((thresh, \n",
    "          metric_original_rf.accuracy(), \n",
    "          metric_original_rf.average_odds_difference(),\n",
    "          metric_original_rf.disparate_impact(),\n",
    "          metric_reweighed_rf.accuracy(), \n",
    "          metric_reweighed_rf.average_odds_difference(),\n",
    "          metric_reweighed_rf.disparate_impact()))\n",
    "\n",
    "report_df = pd.DataFrame(reports)\n",
    "report_df.columns = ['threshhold','accuracy_original',\n",
    "                     'average_odds_difference_original',\n",
    "                     'disparate_impact_original',\n",
    "                     'accuracy_reweighed',\n",
    "                     'average_odds_difference_reweighed',\n",
    "                     'disparate_impact_reweighed']\n",
    "display(report_df.set_index('threshhold'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the deliverable\n",
    "\n",
    "This notebook, complete with your reflections, is your deliverable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
